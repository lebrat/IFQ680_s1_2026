{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44689c92-89df-4a88-8991-57009c91f7fe",
   "metadata": {},
   "source": [
    "# Safety, Performance, and Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84913b40-3370-40c7-8e00-b36d03f9f53d",
   "metadata": {},
   "source": [
    "Let's load in any libraries we will use in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b476050-46bf-4386-b90d-c3f49693a6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "#import torch which has many of the functions to build deep learning models and to train them\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "#import torchvision, which was lots of functions for loading and working with image data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "#this is a nice progress bar representation that will be good to measure progress during training\n",
    "import tqdm\n",
    "\n",
    "#for creating confusion matrices from predictions\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# fix seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# setup device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #this line checks if we have a GPU available\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2f3b8",
   "metadata": {},
   "source": [
    "## 1. Hound species vs. Other species (Binary Classification)\n",
    "\n",
    "In this tutorial, we will build on our previous work on **fine-grained dog breed classification**. However, instead of handling 20 classes, we will focus on a **binary classification problem**, which makes evaluation metrics more intuitive and easier to interpret.  \n",
    "\n",
    "Our goal is to classify **hound species vs. other species**.  \n",
    "\n",
    "> **Definition:** A hound specie is any dog breed with the word `'hound'` in its name.\n",
    "\n",
    "For an image of a dog:\n",
    "\n",
    "- The **probability of being a hound** is the sum of the predicted probabilities of all hound breeds.  \n",
    "- The **probability of not being a hound** is the sum of the predicted probabilities of all other breeds or (1-Prob(hound)).\n",
    "\n",
    "\n",
    "Next, we will **evaluate the performance** of our model on this binary task.  Let's start by setting-up the dataset and trained model for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643dbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "imagenet_means = (0.485, 0.456, 0.406)\n",
    "imagenet_stds = (0.229, 0.224, 0.225)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)), transforms.Normalize(imagenet_means, imagenet_stds)])\n",
    "\n",
    "# create a dataloader\n",
    "batch_size = 16\n",
    "test_dataset = torchvision.datasets.ImageFolder('../Week_3/stanford_dogs_subset/test', transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = 1)\n",
    "print(f\"Test dataset has {len(test_dataset)} samples\")\n",
    "\n",
    "# Load the best model saved in last tutorial\n",
    "resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "in_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 20)\n",
    "resnet.load_state_dict(torch.load(\"../Week_3/resnet_frozen_best.pth\"))\n",
    "resnet = resnet.to(device)\n",
    "print(\"Loaded the best model for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1556ff",
   "metadata": {},
   "source": [
    "**TASK 1:** Now run the model in our test set to collect the ground-truth labels and predicted probabilities for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f686d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect ground-truths, probs and predictions\n",
    "labels, probabilities = [], []\n",
    "\n",
    "...\n",
    "\n",
    "labels, probabilities = np.concatenate(labels).astype(np.int32), np.concatenate(probabilities).astype(np.float32)        \n",
    "print(f\"{len(labels)}/{probabilities.shape} labels and predictions collected\")\n",
    "print(labels[0], probabilities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013dd77",
   "metadata": {},
   "source": [
    "**TASK 2:** Convert collected labels and probabilities to our binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hound classes\n",
    "hound_labels = [idx for idx, name in enumerate(test_dataset.classes) if  'hound' in name]\n",
    "print(f\"Hound class indices: {hound_labels}\")\n",
    "\n",
    "# sum predictions over hound classes\n",
    "probs_hound = ...\n",
    "labels_hound = ...\n",
    "\n",
    "print(f\"Hounds {labels_hound.sum()}/{len(labels_hound)}\")\n",
    "print(labels_hound[0], probs_hound[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52826d76",
   "metadata": {},
   "source": [
    "## Classification Metrics and Evaluation\n",
    "\n",
    "We can use [ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) from sklearn.metrics to create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds  = probs_hound > 0.5\n",
    "accuracy = (labels_hound == preds).mean()\n",
    "ConfusionMatrixDisplay.from_predictions(labels_hound, preds, display_labels=['Other', 'Hound'], xticks_rotation='vertical')\n",
    "plt.title(f'Confusion Matrix (Accuracy: {accuracy*100:.2f}%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa5bbc",
   "metadata": {},
   "source": [
    "Can we code it by ourselves? \n",
    "\n",
    "In a **binary classification** problem, the **confusion matrix** is defined by the following counts:\n",
    "\n",
    "|                  | Predicted Negative | Predicted Positive |\n",
    "|------------------|-------------------|-------------------|\n",
    "| **Actual Negative** | True Negative (TN) | False Positive (FP) |\n",
    "| **Actual Positive** | False Negative (FN) | True Positive (TP) |\n",
    "\n",
    "\n",
    "- TP (True Positive): Model predicts positive, and the true label is positive.\n",
    "- FP (False Positive): Model predicts positive, but the true label is negative.\n",
    "- TN (True Negative): Model predicts negative, and the true label is negative.\n",
    "- FN (False Negative): Model predicts negative, but the true label is positive.\n",
    "\n",
    "OBS: Sometimes this matrix can look transposed.\n",
    "\n",
    "**TASK 3:** Compute these counts using only numpy and plot a confusion matrix like sklearn one using matplotlib: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3126d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_counts(labels, probs, threshold=0.5):\n",
    "    labels, probs = labels.astype(bool), probs.astype(float)\n",
    "\n",
    "    # decision threshold\n",
    "    preds  = ...\n",
    "\n",
    "    # confusion matrix counts\n",
    "    tp = ...\n",
    "    fp = ...\n",
    "    tn = ...\n",
    "    fn =  ...\n",
    "\n",
    "    return tp, fp, tn ,fn\n",
    "\n",
    "\n",
    "# plot confusion matrix with counts\n",
    "tp, fp, tn, fn = confusion_matrix_counts(labels_hound, probs_hound, threshold=0.5)\n",
    "cm_counts = np.array([[tn, fp], [fn, tp]]).astype(np.int32)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "im = ax.imshow(cm_counts, cmap='viridis')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(cm_counts.shape[0]):\n",
    "    for j in range(cm_counts.shape[1]):\n",
    "        ax.text(j, i, str(cm_counts[i, j]),\n",
    "                ha='center', va='center', color='white', fontsize=14)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Others', 'Hounds'])\n",
    "ax.set_yticklabels(['Others', 'Hounds'])\n",
    "ax.set_xlabel('Predicted label')\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Optional: colorbar\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a81b39c",
   "metadata": {},
   "source": [
    "From the confusion matrix, we can compute classification metrics and curves that will help us to evaluate our model:\n",
    "\n",
    "###  Precision\n",
    "Precision measures the proportion of correctly predicted positive samples out of all samples predicted as positive.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{TP}{PP}\n",
    "$$\n",
    "\n",
    "- High precision → very few false positives.  \n",
    "- Example: Out of all dogs predicted to be hound dogs, how many really are?\n",
    "\n",
    "\n",
    "### Recall (Sensitivity, True Positive Rate)\n",
    "Recall measures the proportion of correctly predicted positive samples out of all actual positive samples.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{TP}{P}\n",
    "$$\n",
    "\n",
    "- High recall → very few false negatives.  \n",
    "- Example: Out of all dogs that are actually hound dogs, how many did we detect?\n",
    "\n",
    "\n",
    "###  F-Score (F1-Score)\n",
    "The F-score is the harmonic mean of precision and recall. It balances the two metrics:\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "- High F1 means both precision and recall are reasonably high.  \n",
    "- Useful when there is **class imbalance**.\n",
    "\n",
    "Let's compute these metrics for our hound classification problem.\n",
    "\n",
    "**TASK 4:** From the confusion matrix counts (i.e., tp, fp, tn, fn) computed above, compute precision, recall and f-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute precision, recall, f-score\n",
    "def compute_metrics(labels, probs, threshold=0.5):\n",
    "    # get confusion matrix counts\n",
    "    tp, fp, tn, fn = confusion_matrix_counts(labels, probs, threshold=threshold)\n",
    "    # metrics equations\n",
    "    prec = ...\n",
    "    rec = ...\n",
    "    fscore = ...\n",
    "    return prec, rec, fscore\n",
    "\n",
    "# change threshold here\n",
    "threshold = 0.5\n",
    "prec, rec, fscore = compute_metrics(labels_hound, probs_hound, threshold=threshold)\n",
    "\n",
    "print(f\"Threshold={threshold:.2f}, precision={prec:.2f}, recall={rec:.2f}, f-score={fscore:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e92a95",
   "metadata": {},
   "source": [
    "However, this performance is calculated for a threshold of 0.5. You can re-run the code using different thresholds. By setting different thresholds, we obtain different results. Which threshold should we choose, and how can we compare or evaluate performance across all possible thresholds? What happens with precision and recall metrics as we change the threhold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690fcb9",
   "metadata": {},
   "source": [
    "To answer this question, let's have a look in the distribution of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ee69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram\n",
    "num_bins = 50  # number of bins (e.g., 0.0–0.1, 0.1–0.2, ..., 0.9–1.0)\n",
    "threshold = 0.5 # decision threshold\n",
    "hounds_counts, hounds_bin_edges = np.histogram(probs_hound[labels_hound], bins=num_bins, range=(0, 1))\n",
    "others_counts, others_bin_edges = np.histogram(probs_hound[~labels_hound], bins=num_bins, range=(0, 1))\n",
    "\n",
    "# Plot histogram using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(hounds_bin_edges[:-1], hounds_counts, width=hounds_bin_edges[1] - hounds_bin_edges[0], edgecolor='black', align='edge', label='Hounds', alpha=0.5)\n",
    "plt.bar(others_bin_edges[:-1], others_counts, width=others_bin_edges[1] - others_bin_edges[0], edgecolor='black', align='edge', label='Others', alpha=0.5)\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Frequency (number of samples)')\n",
    "prec, rec, fscore = compute_metrics(labels_hound, probs_hound, threshold=threshold)\n",
    "plt.vlines(x=threshold, ymin=0, ymax=max(max(hounds_counts), max(others_counts)), colors='red', linestyles='dashed', label=f'Decision Threshold = {threshold:.2f}')\n",
    "plt.title(f'Distribution of predicted probabilities (Precision={prec:.2f}, Recall={rec:.2f}, F-score={fscore:.2f})')\n",
    "plt.xticks(np.linspace(0, 1, num_bins + 1), rotation=90)  # ticks at each bin edge\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff3bb58",
   "metadata": {},
   "source": [
    "> **Note:** Selecting a threshold defines how the model decides whether a dog is a hound or not.  \n",
    "\n",
    "Misclassifications occur in two ways:\n",
    "\n",
    "- **False positives (FP):** a non-hound dog receives a probability **higher than the threshold** and is incorrectly classified as a hound.  \n",
    "- **False negatives (FN):** a hound dog receives a probability **lower than the threshold** and is incorrectly classified as not a hound.  \n",
    "\n",
    "Adjusting the threshold **trades off false positives and false negatives**:\n",
    "\n",
    "- Moving the threshold to the **left** increases the number of hounds detected but may also increase false positives.  \n",
    "- Moving the threshold to the **right** reduces false positives but may increase false negatives.  \n",
    "\n",
    "The choice of threshold depends on the **application** — whether it is more important to avoid false negatives or false positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5b55b",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "But how to see this trade off in terms of metrics? An how to evaluate a model across all threholds? The **ROC (Receiver Operating Characteristic) curve** is a tool to evaluate the performance of a **binary classifier** across **all possible thresholds**.  \n",
    "\n",
    "- **x-axis:** False Positive Rate (FPR)  (also called 1−Specificity)\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN} = \\frac{FP}{N}\n",
    "$$  \n",
    "\n",
    "- **y-axis:** True Positive Rate (TPR) (also named sensitivity)  \n",
    "$$\n",
    "\\text{TPR} = \\text{Recall} = \\frac{TP}{TP + FN} = \\frac{TP}{P}\n",
    "$$  \n",
    "\n",
    "- Each point on the curve corresponds to a different **decision threshold** used to convert probabilities into class labels.  \n",
    "\n",
    "- The **diagonal line** represents random guessing; points above it indicate better-than-random performance.\n",
    "\n",
    "- The classifier's performance on the ROC curve is often summarized by a single number: the area under the ROC curve (**AUC-ROC**).\n",
    "\n",
    "- Sklearn provide functions to compute [ROC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) curve and [AUC-ROC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html). But, we can also do by ourselves.\n",
    "\n",
    "**TASK 5:** Complete the code below to plot the ROC curve. Specifically, fill out the loop computing tpr and fpr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort scores and corresponding true labels descending\n",
    "desc_order = np.argsort(-probs_hound)\n",
    "y_true_sorted = labels_hound[desc_order]\n",
    "y_scores_sorted = probs_hound[desc_order]\n",
    "\n",
    "# Initialize counts\n",
    "TP, FP = 0, 0  \n",
    "P, N = np.sum(labels_hound == 1), np.sum(labels_hound == 0)\n",
    "\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "\n",
    "for label in y_true_sorted:\n",
    "    ...\n",
    "    tpr_list.append(...)\n",
    "    fpr_list.append(...)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "tpr = np.array(tpr_list)\n",
    "fpr = np.array(fpr_list)\n",
    "auc = np.trapezoid(tpr, fpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random guess')\n",
    "ths=0.5; pred_ths = probs_hound >= ths;\n",
    "tpr_ths = (pred_ths & labels_hound).sum() / P\n",
    "fpr_ths = (pred_ths & ~labels_hound).sum() / N\n",
    "plt.plot(fpr_ths, tpr_ths, marker='o', color='red', label=f'Threshold = {ths}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00866d01",
   "metadata": {},
   "source": [
    "Likewise, **moving along the ROC curve reflects the trade-off between false positives and false negatives** as the threshold changes.\n",
    "\n",
    "An ideal classifier would be close to the **top-left corner** of the ROC space (high TPR and low FPR).  \n",
    "The choice of operating point (threshold) depends on the **application requirements** and the relative cost of false positives versus false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59cad6",
   "metadata": {},
   "source": [
    "# 3) Classifier Calibration\n",
    "\n",
    "Calibration measures how well a classifier's **predicted probabilities** match the **true likelihood of an event**.  \n",
    "\n",
    "- A perfectly calibrated model outputs probabilities that reflect the actual frequency of positives.  \n",
    "- Example (binary classification):\n",
    "  - If the model predicts **0.7** for 100 samples, roughly **70 of them should belong to the positive class**.\n",
    "  - If it predicts **0.2**, about **20 should be positive**.\n",
    "\n",
    "WARN: High accuracy does not guarantee well-calibrated probabilities, which can make them unreliable for decision-making, risk assessment, or thresholding.\n",
    "\n",
    "\n",
    "Let's plot a calibration curve to check our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bfaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of bins\n",
    "n_bins = 10\n",
    "bins = np.linspace(0, 1, n_bins + 1)\n",
    "\n",
    "bin_indices = np.digitize(probs_hound, bins) - 1  # get bin index for each probability\n",
    "bin_true_frac = []\n",
    "bin_mean_prob = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(n_bins):\n",
    "    mask = bin_indices == i\n",
    "    mask_sum = np.sum(mask)\n",
    "    bin_counts.append(mask_sum)\n",
    "    if mask_sum > 0:\n",
    "        bin_true_frac.append(np.mean(labels_hound[mask]))  # fraction of positives\n",
    "        bin_mean_prob.append(np.mean(probs_hound[mask]))  # mean predicted probability\n",
    "    else:\n",
    "        bin_true_frac.append(np.nan)\n",
    "        bin_mean_prob.append(np.nan)\n",
    "\n",
    "# Convert to arrays\n",
    "bin_true_frac = np.array(bin_true_frac)\n",
    "bin_mean_prob = np.array(bin_mean_prob)\n",
    "bin_counts = np.array(bin_counts)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6,8), sharex=True, gridspec_kw={'height_ratios':[3,1]})\n",
    "\n",
    "# Top: calibration curve\n",
    "ax1.plot(bin_mean_prob, bin_true_frac, marker='o', label='Classifier')\n",
    "ax1.plot([0,1], [0,1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "ax1.set_ylabel('Fraction of positives')\n",
    "ax1.set_title('Calibration Curve with Sample Counts')\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Bottom: histogram of counts\n",
    "ax2.bar(bin_mean_prob, bin_counts, width=0.08, color='gray')\n",
    "ax2.set_xlabel('Mean predicted probability')\n",
    "ax2.set_ylabel('Number of samples')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e3867",
   "metadata": {},
   "source": [
    "### Overconfident and Underconfident Predictions\n",
    "\n",
    "- **Overconfident:** Predicted probabilities are **too high** compared to actual outcomes.  \n",
    "  - Example: The model predicts 0.9, but only 60% of samples are truly positive.  \n",
    "  - Calibration Curve **below** the diagonal\n",
    "- **Underconfident:** Predicted probabilities are **too low** compared to actual outcomes.  \n",
    "  - Example: The model predicts 0.4, but 70% of samples are truly positive.  \n",
    "  - Calbration Curve **above** the diagonal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97a6cb",
   "metadata": {},
   "source": [
    "# Extending Binary Metrics to Multi-class Classification\n",
    "\n",
    "So far, we have discussed metrics, ROC curves, and calibration in the context of **binary classification**.  \n",
    "\n",
    "**Question:**  \n",
    "How could you adapt these concepts to a **multi-class classification problem**? Consider:\n",
    "\n",
    "- How would you compute precision, recall, and F1-score when there are more than two classes?  \n",
    "- How can ROC curves and AUC be extended beyond binary problems?  \n",
    "- How could you assess and visualize calibration for multiple classes?\n",
    "\n",
    "Try to think about strategies like **one-vs-rest**, **macro vs. micro averaging**, and grouping classes.  \n",
    "\n",
    "**Reading:**  \n",
    "- [scikit-learn: Multiclass classification Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)  \n",
    "- [scikit-learn: ROC curves for multiclass](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)  \n",
    "- [scikit-learn: Calibration of classifiers](https://scikit-learn.org/stable/modules/calibration.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ec9a9",
   "metadata": {},
   "source": [
    "# Saliency Maps with Class Activation Mapping (CAM)\n",
    "\n",
    "Saliency maps help us understand **which regions of an image most influence a model’s prediction**. They are useful for interpreting and debugging CNN-based image classifiers.\n",
    "\n",
    "Class Activation Mapping (CAM) is a saliency method that uses the **feature maps from the last convolutional layer** of a CNN. These feature maps capture high-level visual patterns (e.g. shapes or object parts). The final classification layer assigns **class-specific weights** to these features, and combining them produces a **class-specific heatmap** showing which regions contributed most to the prediction.\n",
    "\n",
    "![CAM.png](CAM.png)\n",
    "\n",
    "The resulting CAM highlights **where the model is “looking”** when predicting a particular class.\n",
    "\n",
    "CAM is useful for:\n",
    "- Checking whether the model focuses on the **relevant object** rather than the background.\n",
    "- Gaining insight into **why a class was predicted**.\n",
    "- Identifying reliance on **spurious visual cues**.\n",
    "\n",
    "> **Note:** Standard CAM requires a CNN with a **global average pooling (GAP)** layer followed by a linear classifier. Methods such as **Grad-CAM** relax this requirement and can be applied to a wider range of architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a few images\n",
    "# print(test_dataset.class_to_idx)\n",
    "selected_label = ['basset', 'bloodhound', 'Maltese_dog']\n",
    "selected_indices = []\n",
    "for class_name in selected_label:\n",
    "    class_idx = test_dataset.class_to_idx[class_name]\n",
    "    class_samples = [idx for idx, sample in enumerate(test_dataset.samples) if sample[1] == class_idx]\n",
    "    selected_indices.append(np.random.choice(class_samples, 1).tolist()[0])\n",
    "print(f\"Selected labels: {selected_label} - {[test_dataset.class_to_idx[class_name] for class_name in selected_label]}\")\n",
    "print(f\"Selected indices: {selected_indices}\")\n",
    "\n",
    "# Load and display the selected images\n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 5))\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    x, y = test_dataset[idx]\n",
    "    x_display = x.cpu().numpy().transpose(1, 2, 0) * imagenet_stds + imagenet_means\n",
    "    x_display = np.clip(x_display, 0, 1)\n",
    "    \n",
    "    axs[i].imshow(x_display)\n",
    "    axs[i].set_title(f'{test_dataset.classes[y]}')\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84145c",
   "metadata": {},
   "source": [
    "**TASK 6:** Implement the class activation mapping computation in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d47dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "topk = 1\n",
    "fig, axs = plt.subplots(3,topk+1, figsize=(5*topk+1,8))\n",
    "resnet.layer4[1].register_forward_hook(get_activation('resnet.layer4.1.conv2'))\n",
    "resnet.eval()\n",
    "with torch.no_grad():\n",
    "    for i, sel_idx in enumerate(selected_indices): \n",
    "        x, y = test_dataset[sel_idx]\n",
    "        x = x.to(device)\n",
    "\n",
    "        # plot original image\n",
    "        x_display = x.detach().cpu().numpy().transpose(1, 2, 0) * imagenet_stds + imagenet_means\n",
    "        x_display = np.clip(x_display, 0, 1)  # Clip to valid range [0, 1]\n",
    "        axs[i, 0].imshow(x_display)       \n",
    "        axs[i, 0].set_title(f'{test_dataset.classes[y]}'); axs[i, 0].axis('off');\n",
    "        \n",
    "        # get top-k predictions and conv feature maps\n",
    "        output = resnet(x.unsqueeze(0))\n",
    "        topk_preds = torch.topk(output, k=1, dim=1).indices  # top-5 predictions\n",
    "        conv_feature_map = activation['resnet.layer4.1.conv2'][0].detach()\n",
    "        \n",
    "        # compute class activation maps for top-5 predictions\n",
    "        for j, pred in enumerate(topk_preds[0]):\n",
    "            fc_weights = resnet.state_dict()['fc.weight'][pred].detach()\n",
    "            \n",
    "            # compute CAM\n",
    "            cam = ...\n",
    "            \n",
    "            # plot CAM\n",
    "            axs[i, j+1].imshow(x_display)\n",
    "            axs[i, j+1].imshow(cam, alpha=0.5, cmap='jet')\n",
    "            axs[i, j+1].set_title(f'CAM({test_dataset.classes[pred]})')\n",
    "            axs[i, j+1].axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifn680",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
